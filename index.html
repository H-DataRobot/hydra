
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Welcome to Inventory Performance (IPM)’s documentation &#8212; Inventory Performance (IPM) V 0.1 documentation</title>
    <link rel="stylesheet" href="_static/classic.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    
    <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="_static/language_data.js"></script>
    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="#">Inventory Performance (IPM) V 0.1 documentation</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="welcome-to-inventory-performance-ipm-s-documentation">
<h1>Welcome to Inventory Performance (IPM)’s documentation<a class="headerlink" href="#welcome-to-inventory-performance-ipm-s-documentation" title="Permalink to this headline">¶</a></h1>
<div class="toctree-wrapper compound">
</div>
</div>
<div class="section" id="indices-and-tables">
<h1>Indices and tables<a class="headerlink" href="#indices-and-tables" title="Permalink to this headline">¶</a></h1>
<ul class="simple">
<li><p><a class="reference internal" href="genindex.html"><span class="std std-ref">Index</span></a></p></li>
<li><p><a class="reference internal" href="py-modindex.html"><span class="std std-ref">Module Index</span></a></p></li>
<li><p><a class="reference internal" href="search.html"><span class="std std-ref">Search Page</span></a></p></li>
</ul>
</div>
<div class="section" id="module-utils.monitor_io">
<span id="auto-generated-documentation"></span><h1>Auto Generated Documentation<a class="headerlink" href="#module-utils.monitor_io" title="Permalink to this headline">¶</a></h1>
<dl class="function">
<dt id="utils.monitor_io.consolidated_dict">
<code class="sig-prename descclassname">utils.monitor_io.</code><code class="sig-name descname">consolidated_dict</code><span class="sig-paren">(</span><em class="sig-param">spark: pyspark.sql.session.SparkSession</em><span class="sig-paren">)</span> &#x2192; Dict[str, pyspark.sql.dataframe.DataFrame]<a class="headerlink" href="#utils.monitor_io.consolidated_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>This are the input dataframes for the consolidated dag.
:param spark: Spark session
:return: Multiple dataframes in a dictionary needed for the process.
TODO Migrate to hive table once available</p>
</dd></dl>

<dl class="function">
<dt id="utils.monitor_io.feature_dict">
<code class="sig-prename descclassname">utils.monitor_io.</code><code class="sig-name descname">feature_dict</code><span class="sig-paren">(</span><em class="sig-param">spark: pyspark.sql.session.SparkSession</em><span class="sig-paren">)</span> &#x2192; Dict[str, pyspark.sql.dataframe.DataFrame]<a class="headerlink" href="#utils.monitor_io.feature_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>This are the input dataframes for the Feature dag.
:param spark: Spark session
:return: Multiple dataframes in a dictionary needed for the process.
TODO Migrate to hive table once available</p>
</dd></dl>

<dl class="function">
<dt id="utils.monitor_io.finance_controller_dict">
<code class="sig-prename descclassname">utils.monitor_io.</code><code class="sig-name descname">finance_controller_dict</code><span class="sig-paren">(</span><em class="sig-param">spark: pyspark.sql.session.SparkSession</em><span class="sig-paren">)</span> &#x2192; Dict[str, pyspark.sql.dataframe.DataFrame]<a class="headerlink" href="#utils.monitor_io.finance_controller_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the input dataframes required for the qc process.
:param spark: Spark session
:return: Multiple dataframes in a dictionary needed for the process.</p>
</dd></dl>

<dl class="function">
<dt id="utils.monitor_io.finance_controller_hist_dict">
<code class="sig-prename descclassname">utils.monitor_io.</code><code class="sig-name descname">finance_controller_hist_dict</code><span class="sig-paren">(</span><em class="sig-param">spark: pyspark.sql.session.SparkSession</em><span class="sig-paren">)</span> &#x2192; Dict[str, pyspark.sql.dataframe.DataFrame]<a class="headerlink" href="#utils.monitor_io.finance_controller_hist_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the input dataframes required for the qc process.
:param spark: Spark session
:return: Multiple dataframes in a dictionary needed for the process.</p>
</dd></dl>

<dl class="function">
<dt id="utils.monitor_io.get_qualilty_dict">
<code class="sig-prename descclassname">utils.monitor_io.</code><code class="sig-name descname">get_qualilty_dict</code><span class="sig-paren">(</span><em class="sig-param">spark: pyspark.sql.session.SparkSession</em><span class="sig-paren">)</span> &#x2192; Dict[str, pyspark.sql.dataframe.DataFrame]<a class="headerlink" href="#utils.monitor_io.get_qualilty_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the input dataframes required for the qc process.
:param spark: Spark session
:return: Multiple dataframes in a dictionary needed for the process.</p>
</dd></dl>

<dl class="function">
<dt id="utils.monitor_io.historic_dict">
<code class="sig-prename descclassname">utils.monitor_io.</code><code class="sig-name descname">historic_dict</code><span class="sig-paren">(</span><em class="sig-param">spark: pyspark.sql.session.SparkSession</em><span class="sig-paren">)</span> &#x2192; Dict[str, pyspark.sql.dataframe.DataFrame]<a class="headerlink" href="#utils.monitor_io.historic_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>This are the input dataframes for the sap bw historic dag.
:param spark: Spark session
:return: Multiple dataframes in a dictionary needed for the process.
TODO Migrate to hive table once available</p>
</dd></dl>

<dl class="function">
<dt id="utils.monitor_io.stock_projection_dict">
<code class="sig-prename descclassname">utils.monitor_io.</code><code class="sig-name descname">stock_projection_dict</code><span class="sig-paren">(</span><em class="sig-param">spark: pyspark.sql.session.SparkSession</em><span class="sig-paren">)</span> &#x2192; Dict[str, pyspark.sql.dataframe.DataFrame]<a class="headerlink" href="#utils.monitor_io.stock_projection_dict" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns the input dataframes required for the qc process.
:param spark: Spark session
:return: Multiple dataframes in a dictionary needed for the process.</p>
</dd></dl>

<span class="target" id="module-utils.common_functions"></span><dl class="function">
<dt id="utils.common_functions.cast_columns">
<code class="sig-prename descclassname">utils.common_functions.</code><code class="sig-name descname">cast_columns</code><span class="sig-paren">(</span><em class="sig-param">dataframe</em>, <em class="sig-param">col_cast_type_dict</em><span class="sig-paren">)</span><a class="headerlink" href="#utils.common_functions.cast_columns" title="Permalink to this definition">¶</a></dt>
<dd><p>Function to cast columns in a dataframe. written to reduce the need for multiple withColumn commands
:param df: df where to cast columns
:param col_cast_type_dict: Dictionary of column and their expected datatype. e.g. {‘column_1’: (‘timestamp’,
‘yyyy.MM.dd HH:mm:ss’), ‘column_2 : (int,None)}
here column_1 is to be casted as timestamp with the given format and column2  to be casted to int
:return: Dataframe with casted values</p>
</dd></dl>

<dl class="function">
<dt id="utils.common_functions.deduplicate_atlas_table">
<code class="sig-prename descclassname">utils.common_functions.</code><code class="sig-name descname">deduplicate_atlas_table</code><span class="sig-paren">(</span><em class="sig-param">dataframe</em>, <em class="sig-param">partition_by_list</em>, <em class="sig-param">order_by_list</em><span class="sig-paren">)</span><a class="headerlink" href="#utils.common_functions.deduplicate_atlas_table" title="Permalink to this definition">¶</a></dt>
<dd><p>This function performs the deduplication logic for atlas tables
:param df: Atlas dataframe to be deduplicated
:param partition_by_list: List of columns to partition by
:param order_by_list:  List fo tupples e.g. [(column_1,0),(column_2,1)]. here 0 signifies descending order,
1 is ascending
:return: Deduplicated atlas dataframe</p>
</dd></dl>

<dl class="function">
<dt id="utils.common_functions.flatten_df">
<code class="sig-prename descclassname">utils.common_functions.</code><code class="sig-name descname">flatten_df</code><span class="sig-paren">(</span><em class="sig-param">nested_df</em><span class="sig-paren">)</span><a class="headerlink" href="#utils.common_functions.flatten_df" title="Permalink to this definition">¶</a></dt>
<dd><p>this is used flattten a nested dataframe works for only 1 level of nesting
:param nested_df:
:return:</p>
</dd></dl>

<dl class="function">
<dt id="utils.common_functions.historic_collection_check">
<code class="sig-prename descclassname">utils.common_functions.</code><code class="sig-name descname">historic_collection_check</code><span class="sig-paren">(</span><em class="sig-param">spark: pyspark.sql.session.SparkSession</em><span class="sig-paren">)</span> &#x2192; bool<a class="headerlink" href="#utils.common_functions.historic_collection_check" title="Permalink to this definition">¶</a></dt>
<dd><p>checks if the historic data needs to be recollected
:param spark: spark Session
:param calendar: mars calendar
:return: true/ false</p>
</dd></dl>

<dl class="function">
<dt id="utils.common_functions.historic_selective_reading">
<code class="sig-prename descclassname">utils.common_functions.</code><code class="sig-name descname">historic_selective_reading</code><span class="sig-paren">(</span><em class="sig-param">date_list: list</em>, <em class="sig-param">date_tup: tuple</em>, <em class="sig-param">history_df: pyspark.sql.dataframe.DataFrame</em>, <em class="sig-param">table_name: str</em><span class="sig-paren">)</span><a class="headerlink" href="#utils.common_functions.historic_selective_reading" title="Permalink to this definition">¶</a></dt>
<dd><p>Function to read every period’s data from the end week of the last period
:param date_list: saturday dates for last 2 years
:param date_tup: date indicator column for various files
:param history_df: dataframe with historical stock data
:param table_name: name of the table which is processed
:return: dataframe with selectively read files</p>
</dd></dl>

<dl class="function">
<dt id="utils.common_functions.period_end_sat_files">
<code class="sig-prename descclassname">utils.common_functions.</code><code class="sig-name descname">period_end_sat_files</code><span class="sig-paren">(</span><em class="sig-param">mars_calendar: pyspark.sql.dataframe.DataFrame</em><span class="sig-paren">)</span><a class="headerlink" href="#utils.common_functions.period_end_sat_files" title="Permalink to this definition">¶</a></dt>
<dd><p>return the dates from which files are to be read for the historic finance files
:param mars_calendar:
:param min_prd:
:return:</p>
</dd></dl>

<dl class="function">
<dt id="utils.common_functions.read_the_data">
<code class="sig-prename descclassname">utils.common_functions.</code><code class="sig-name descname">read_the_data</code><span class="sig-paren">(</span><em class="sig-param">item: str</em>, <em class="sig-param">delimiter: str</em>, <em class="sig-param">file_format: str</em>, <em class="sig-param">treat_column: bool = False</em><span class="sig-paren">)</span> &#x2192; pyspark.sql.dataframe.DataFrame<a class="headerlink" href="#utils.common_functions.read_the_data" title="Permalink to this definition">¶</a></dt>
<dd><p>function:- Function that can be used across the projects to load a single file
:param item: file path from constants
:param delimiter: delimiter of file
:param file_format: format of the file i.e. parquet csv etc
:return: dataframe loaded from single file</p>
</dd></dl>

<dl class="function">
<dt id="utils.common_functions.read_the_historical_data">
<code class="sig-prename descclassname">utils.common_functions.</code><code class="sig-name descname">read_the_historical_data</code><span class="sig-paren">(</span><em class="sig-param">item_historic: str</em>, <em class="sig-param">delimiter: str</em>, <em class="sig-param">file_format: str</em><span class="sig-paren">)</span> &#x2192; pyspark.sql.dataframe.DataFrame<a class="headerlink" href="#utils.common_functions.read_the_historical_data" title="Permalink to this definition">¶</a></dt>
<dd><p>function:- Function that can be used across the projects to load a
single file- not used as moved to using historic tables
Note- This function is not tested
:param item_historic: file path from constants * all directories
:param delimiter: delimiter of file
:param file_format: format of the file i.e. paraquet csv etc
:return: dataframe loaded from multiple file</p>
</dd></dl>

<dl class="function">
<dt id="utils.common_functions.retrieve_name">
<code class="sig-prename descclassname">utils.common_functions.</code><code class="sig-name descname">retrieve_name</code><span class="sig-paren">(</span><em class="sig-param">var</em><span class="sig-paren">)</span><a class="headerlink" href="#utils.common_functions.retrieve_name" title="Permalink to this definition">¶</a></dt>
<dd><p>Gets the name of var. Does it from the out most frame inner-wards.
:param var: variable to get name from.
:return: string</p>
</dd></dl>

<dl class="function">
<dt id="utils.common_functions.to_long">
<code class="sig-prename descclassname">utils.common_functions.</code><code class="sig-name descname">to_long</code><span class="sig-paren">(</span><em class="sig-param">df</em>, <em class="sig-param">by</em><span class="sig-paren">)</span><a class="headerlink" href="#utils.common_functions.to_long" title="Permalink to this definition">¶</a></dt>
<dd><p>convert the table to long format form wide format
:param df: dataframe to be converted to long format
:param by: list of columns on which pivoting of other columns happens
:return: dataframe in a long format</p>
</dd></dl>

<dl class="function">
<dt id="utils.common_functions.union_by_name">
<code class="sig-prename descclassname">utils.common_functions.</code><code class="sig-name descname">union_by_name</code><span class="sig-paren">(</span><em class="sig-param">df_list: list</em><span class="sig-paren">)</span> &#x2192; pyspark.sql.dataframe.DataFrame<a class="headerlink" href="#utils.common_functions.union_by_name" title="Permalink to this definition">¶</a></dt>
<dd><p>union all the Data Frames by column names. create empty column if a column is missing in a dataframe
:param df_list: list of dataframes to union
:return: consolidated dataframe dataframe</p>
</dd></dl>

<dl class="function">
<dt id="utils.common_functions.write_hive_overwrite">
<code class="sig-prename descclassname">utils.common_functions.</code><code class="sig-name descname">write_hive_overwrite</code><span class="sig-paren">(</span><em class="sig-param">dataframe_name</em>, <em class="sig-param">database_name</em>, <em class="sig-param">write_loc</em><span class="sig-paren">)</span><a class="headerlink" href="#utils.common_functions.write_hive_overwrite" title="Permalink to this definition">¶</a></dt>
<dd><blockquote>
<div><p>This function overwrites the external hive table, at given location i.e. if
hive table is dropped by mistake it would still hold the file in lake.</p>
</div></blockquote>
<dl class="field-list simple">
<dt class="field-odd">Parameters</dt>
<dd class="field-odd"><p><strong>dataframe_name</strong> – <dl class="simple">
<dt>Dataframe which gets written to hive table. It is passed</dt><dd><p>as a list from constant file and multiple Dataframes gets written based on the Dag.</p>
</dd>
</dl>
<dl class="field-list simple">
<dt class="field-odd">param write_loc</dt>
<dd class="field-odd"><p>Location to write in lake
:return: None</p>
</dd>
</dl>
</p>
</dd>
</dl>
</dd></dl>

<dl class="function">
<dt id="utils.common_functions.write_the_table_append">
<code class="sig-prename descclassname">utils.common_functions.</code><code class="sig-name descname">write_the_table_append</code><span class="sig-paren">(</span><em class="sig-param">dataframe_name</em>, <em class="sig-param">write_loc</em><span class="sig-paren">)</span><a class="headerlink" href="#utils.common_functions.write_the_table_append" title="Permalink to this definition">¶</a></dt>
<dd><p>This function appends the file in datalake - not used anymore as process
moved to reading hive tables.
:param dataframe_name: Dataframe which gets written to hive table. It is passed
as a list from constant file and multiple Dataframes gets written based on the Dag.
:param write_loc: Location to write in lake
:return: None</p>
</dd></dl>

<dl class="function">
<dt id="utils.common_functions.write_the_table_overwrite">
<code class="sig-prename descclassname">utils.common_functions.</code><code class="sig-name descname">write_the_table_overwrite</code><span class="sig-paren">(</span><em class="sig-param">dataframe_name</em>, <em class="sig-param">write_loc</em><span class="sig-paren">)</span><a class="headerlink" href="#utils.common_functions.write_the_table_overwrite" title="Permalink to this definition">¶</a></dt>
<dd><p>This function overwrites the file in datalake - not used anymore as process
moved to reading hive tables.
:param dataframe_name: Dataframe which gets written to hive table. It is passed
as a list from constant file and multiple Dataframes gets written based on the Dag.
:param write_loc: Location to write in lake
:return: None</p>
</dd></dl>

</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h3><a href="#">Table of Contents</a></h3>
  <ul>
<li><a class="reference internal" href="#">Welcome to Inventory Performance (IPM)’s documentation</a></li>
<li><a class="reference internal" href="#indices-and-tables">Indices and tables</a></li>
<li><a class="reference internal" href="#module-utils.monitor_io">Auto Generated Documentation</a></li>
</ul>

  <div role="note" aria-label="source link">
    <h3>This Page</h3>
    <ul class="this-page-menu">
      <li><a href="_sources/index.rst.txt"
            rel="nofollow">Show Source</a></li>
    </ul>
   </div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="#">Inventory Performance (IPM) V 0.1 documentation</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2019, Hardik Talati.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 2.1.2.
    </div>
  </body>
</html>
